\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{enumerate}
\usepackage{centernot}
\usepackage[margin=1in]{geometry}

\newcommand{\p}[1]{\left(#1\right)}
\newcommand{\f}[2]{\frac{#1}{#2}}
\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}

\newcommand{\E}{\mathbb{E}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\Var}{\mathrm{Var}}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\s}[1]{\left[#1\right]}
\renewcommand{\d}{\mathrm{d}}
\newcommand{\eps}{\varepsilon}

\begin{document}

	\section*{LLT vs CLT}
		CLT talks about the probability of a random variable $X$ being within an interval [a,b].
		LLT instead talks about pointwise probability.
		CLT: 
\[ \abs{P(X_n < t) - P(Z < t)} \xrightarrow{n \to \infty} 0 \]

LLT: For normalized X_n,
\[ P(X_n = k) = \Phi(k) + o(1) \text{ where } \Phi(k) = \f{1}{\sqrt{2\pi}\sigma_n} \mathrm{exp}\p{-((k-\mu_n)/\sigma_n)^2/2} \]
		Essentially, we have to show \[ \abs{P(X_n = k) - P(Z = k) }=  o(1)\].
To establish a local limit theorem, we want to prove a quantitative bound for $\int_{-\pi\sigma_n}^{\pi\sigma_n} \abs{\phi_n(t) - e^{-t^2/2}} dt$.		
		
		Why does a CLT not give us an LLT for free? \\
		The cumulative distribution function (CDF) of the standard normal = $\Phi(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{x} e^{-t^2/2} dt$.
		CLT implies $\abs{P(a\leq x\leq b - (\Phi(b) - \Phi(a))} < \epsilon(n) \rightarrow 0$ as $n \rightarrow \infty$.
		For an LLT: $P(x = k) = \abs{P(k-\frac{1}{2} \leq x \leq k+\frac{1}{2} - (\Phi(k+\frac{1}{2}) - \Phi(k-\frac{1}{2}))} \leq \epsilon(n)$ (from CLT). \\
		We know the probability $P(x=k)$ is small, so we want to get a bound $\epsilon(n)$ that is actually meaningful.
		Notice the area under the normal distribution = 1, so scaling the x-axis by standard deviation $\sigma$ implies scaling the y-axis by $\frac{1}{\sigma}$. Thus, for a meaningful LLT (that actually implies a small error between the distributions), we need $\epsilon(n) < o(\frac{1}{\sigma})$. 
	
		\subsection*{Intro to Variables Studied}
		To get some more intuition about LLTs, let's look at three random variables we've been studying:
		
		1. Number of triangles in a random graph $G(n,p)$: include each of $n\choose2$ possible edges with probability $p$
		$T_n$ = number of triangles in $G(n,p)$
		Gilmer Kopparty:
		Berkowitz:
		
		2. Number of descents in a permutation:
		Define $D_n = \abs{\{i:1 \leq i \leq n-1, \pi(i+1) < \pi(i) \}}$.
		
		Simulation: n = ?? 
		We observe that the distribution $D_n$ looks like a normal distribution "pointwise," thus we believe (and will later prove) an LLT exists for $D_n$. 
		
		Compare to: \\
		3. $A_n$ = number arithmetic progressions in a random subset $S$ of $\F_n$, with $n>3$, prime; (each element of $\F_n$ has fixed probability $p$ of being in $S$.
	 	R.V. $A_n$ can be thought of as a function $A_n : [0,1]^n \rightarrow \N$. 
$A_n = \f{1}{2} \sum_{k \in \F_n} \sum_{j \neq 0 \in \F_n} X_k X_{k+j} X_{k+2j}$ for $n$ prime and not $2$ or $3$.	 	
	 	\\
	 	Simulation: n = ?? Sample size = ??
	 	See oscillations, within a certain interval the distribution looks normal, but not pointwise: at a certain point, the distance between the normal distribution and distribution of $A_n$ does not converge to 0. Thus, we conjecture there is no LLT, which we will talk about more towards the end of this talk.



	\section*{Characteristic Functions}


	\section*{Descents}

	This proof is ours. The first thing to do when studying descents is to find a good way to sample them. We pick $1 \leq a_i \leq n - i + 1$ and pick $\pi(i)$ to be the $a_i$th smallest integer between $1$ and $n$ that has not already been picked. Then $\pi(i) > \pi(i+1)$ if and only if $a_i > a_{i+1}$. Call this event $X_i$. Some observations:

	\begin{enumerate}[$\bullet$]
		\item
			$D_n = \sum\limits_{i=1}^{n-1} X_i$
		\item
			$\E X_i = 1/2$ (show with picture and duality), $\E D_n = \f{n-1}{2}$
		\item
			Not immediate: $\Var(D_n) = \f{n+1}{12}$
		\item
			Each $X_i$ depends only on its neighbors, $X_{i-1}$ and $X_{i+1}$
	\end{enumerate}

	We want to make $D_n$ look like a sum of independent random variables, so we try to only look at every other term in the sum. To do this, we first condition on the values of all the odd $X_i$, then look at the remaining odd $X_i$. The following facts are very helpful:
	\begin{align*}
		\P(X_i = 1 \mid X_{i-1} = 1,\ X_{i+1} = 1) &= 1/6, \\
		\P(X_i = 1 \mid X_{i-1} = 1,\ X_{i+1} = 0) &= 1/2, \\
		\P(X_i = 1 \mid X_{i-1} = 0,\ X_{i+1} = 1) &= 1/2, \\
		\P(X_i = 1 \mid X_{i-1} = 0,\ X_{i+1} = 0) &= 5/6.
	\end{align*}
	The fact that these are all constants makes the distribution very easy to deal with after conditioning on the $X_i$. We split up the characteristic function into two steps of expectation:
	\[ \phi_n(t) = \E[e^{itD_n/\sigma}] = \underset{\text{odd}}{\E} \ \underset{\text{even}}{\E} [e^{itD_n/\sigma}]. \]

	We leave the conditioning on the even $X_i$ implicit and focus on the inner expectation, using the fact that the odd $X_i$ are independent:

	\[
		\abs{\E[e^{itD_n/\sigma}]}
		= \abs{\E[e^{it(C + \sum\limits_{\text{odd } i} X_i/\sigma}]}
		= \prod_{\text{odd i}} \abs{\E[e^{itX_i/\sigma}]}
	\]

	Now, we need a lemma straight from Gilmer and Kopparty that is mildly annoying to prove. Let $B$ be a Bernoulli random variable that is $1$ with probability $p$ and $0$ with probability $1-p$. Then for $\abs{x} \leq \pi$,
	\[ \abs{\E[e^{ixB}]} \leq 1 - 8p(1-p) \p{\f{x}{2\pi}}^2. \]
	Here, $x = t/\sigma$, so we need $\abs{t} \leq \pi \sigma$. Luckily, these are exactly the values of $t$ that we care about. Each odd $X_i$ has one of four Bernoulli distributions based on the values of $X_{i-1}$ and $X_{i+1}$. Out of these four values of $p$, one of them must minimize $8p(1-p)$; call this minimum value $C$. Then since $\sigma = \Theta(\sqrt{n})$, we get
	\[ \abs{\E[e^{itX_i/\sigma}]} \leq 1 - C\p{\f{t}{2\pi \sigma}}^2 = 1 - \Theta(t^2/n). \]
	Going back, we get
	\[
		\abs{\E[e^{itD_n/\sigma}]}
		\leq \prod_{\text{odd } i} (1 - \Theta(t^2/n))
		\leq (1 - \Theta(t^2/n))^{n/2}
		\leq e^{-\Theta(t^2)/2}
		= e^{-\Theta(t^2)}.
	\]
	This bound holds for any values of the even $X_i$, so it holds in general. To finish the proof, for any constant $A$ we can write
	\[
		\int_{-\pi \sigma}^{\pi \sigma} \abs{\phi_n(t) - e^{-t^2/2}} \d t
		\leq \int_{-A}^{A} \abs{\phi_n(t) - e^{-t^2/2}} \d t
		+ \int_{A \leq \abs{t} \leq \pi \sigma} \underbrace{(\abs{\phi_n(t)} + |e^{-t^2/2}|)}_{e^{-\Theta(t^2)}} \d t
	\]
	We can make the second integral arbitrarily small by picking $A$ to be arbitrarily large, and for any fixed $A$ the first integral goes to $0$ as $n$ gets large. Thus, the whole integral goes to $0$ as $n$ gets large and the local limit theorem for descents is proved.


	\section*{Arithmetic Progressions}


\end{document}
